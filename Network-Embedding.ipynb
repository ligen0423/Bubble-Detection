{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    " \n",
    "# 定义一个简单的Transformer模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('DynamicNetwork.csv',header = None).values\n",
    "df_tensor = torch.tensor(df,dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.    , 0.0431, 0.8815, 0.8103, 1.    , 0.0431, 0.8815, 0.8103,\n",
       "       0.4223, 1.    , 0.6499, 0.862 , 0.4223, 1.    , 0.6499, 0.862 ,\n",
       "       0.2513, 0.0492, 1.    , 0.3717, 0.2513, 0.0492, 1.    , 0.3717,\n",
       "       0.0077, 0.0126, 0.2778, 1.    , 0.0077, 0.0126, 0.2778, 1.    ,\n",
       "       1.    , 0.0431, 0.8815, 0.8103, 1.    , 0.0431, 0.8815, 0.8103,\n",
       "       0.4223, 1.    , 0.6499, 0.862 , 0.4223, 1.    , 0.6499, 0.862 ,\n",
       "       0.2513, 0.0492, 1.    , 0.3717, 0.2513, 0.0492, 1.    , 0.3717,\n",
       "       0.0077, 0.0126, 0.2778, 1.    , 0.0077, 0.0126, 0.2778, 1.    ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(data):\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            if data[i][j] == 0:\n",
    "                data[i][j] = 1\n",
    "    return np.abs(data).reshape(data.shape[0],8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    E = []\n",
    "    for i in range(X.shape[0]):\n",
    "        P = []\n",
    "        for j in range(X.shape[1]):\n",
    "            if i !=j:\n",
    "                e = -X[i][j]*np.log(X[i][j])\n",
    "                P.append(e)\n",
    "        P = np.array(P)\n",
    "        E.append(np.sum(P))\n",
    "    return np.array(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphentropy(X):\n",
    "    E = []\n",
    "    for i in range(X.shape[0]):\n",
    "        e = entropy(X[i])\n",
    "        E.append(np.sum(e))\n",
    "    return np.array(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x,y):\n",
    "    distance = np.mean(np.power((x - y),2))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatrix(data,E):\n",
    "    Matrix = []\n",
    "    for i in range(E.shape[0]):\n",
    "        dis = []\n",
    "        for j in range(E.shape[0]):\n",
    "            dis.append(distance(E[i],E[j]))\n",
    "        dis = np.array(dis)\n",
    "        index = np.argsort(dis)[1]\n",
    "        Matrix.append(data[index])\n",
    "    return np.array(Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = graphentropy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixX = getMatrix(df,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tensor     = torch.tensor(df, dtype = torch.float32)\n",
    "Matrix_tensor = torch.tensor(MatrixX, dtype = torch.float32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        Q = self.q_linear(query).view(batch_size, -1, self.n_head, self.d_model // self.n_head).transpose(1, 2)  # (batch_size, n_head, seq_len, d_model/n_head)\n",
    "        K = self.k_linear(key).view(batch_size, -1, self.n_head, self.d_model // self.n_head).transpose(1, 2)  # (batch_size, n_head, seq_len, d_model/n_head)\n",
    "        V = self.v_linear(value).view(batch_size, -1, self.n_head, self.d_model // self.n_head).transpose(1, 2)  # (batch_size, n_head, seq_len, d_model/n_head)\n",
    "\n",
    "        score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_model / self.n_head)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9)\n",
    "        score = F.softmax(score, dim=-1)\n",
    "        output = torch.matmul(score, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = SelfAttention(d_model, n_head)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        src2 = self.self_attn(src, src, src, mask)\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(F.relu(self.linear1(src)))\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers,d_model):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.pe = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self, src, mask=None):\n",
    "        src = self.pe(src)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 8\n",
    "n_head = 1\n",
    "encoder_layer = TransformerEncoderLayer(d_model, n_head)\n",
    "encoder = TransformerEncoder(encoder_layer, num_layers=6, d_model = d_model)\n",
    "src = torch.rand(10, 32, d_model)\n",
    "output = encoder(df_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2084,  0.5347, -0.5083,  ..., -1.2946, -0.1195, -0.8557],\n",
       "         [ 0.5662,  0.5660, -0.6121,  ..., -0.8518, -0.2084, -0.9177],\n",
       "         [ 0.7936, -0.7140, -0.4073,  ..., -1.2953, -0.2157, -0.7398],\n",
       "         ...,\n",
       "         [ 0.0549,  0.5111, -0.7392,  ..., -0.9332, -0.0617, -0.9952],\n",
       "         [-0.4467,  0.2132,  0.0611,  ..., -1.0955, -0.2136, -1.2274],\n",
       "         [ 0.4209,  0.2480, -0.4988,  ..., -0.9579, -0.5270, -0.8982]],\n",
       "\n",
       "        [[ 0.2535,  0.5675, -0.7622,  ..., -1.3451, -0.4194, -0.5255],\n",
       "         [ 0.7244,  0.2664, -0.6460,  ..., -0.8945, -0.1952, -0.8838],\n",
       "         [ 0.5979, -0.6546, -0.3460,  ..., -1.3370, -0.3843, -0.6283],\n",
       "         ...,\n",
       "         [-0.1214,  0.6293, -0.5810,  ..., -0.8245, -0.4508, -0.9992],\n",
       "         [-0.3745,  0.3311,  0.1021,  ..., -1.4268, -0.0949, -1.1154],\n",
       "         [ 0.6697,  0.3268, -0.7203,  ..., -0.9297, -0.7516, -0.8998]],\n",
       "\n",
       "        [[ 0.2099,  0.8953, -0.9504,  ..., -1.1256, -0.1553, -0.8568],\n",
       "         [ 0.5331,  0.8903, -0.7159,  ..., -0.8876, -0.2842, -1.0644],\n",
       "         [ 0.4644, -0.8685, -0.3771,  ..., -1.6637, -0.1836, -0.2142],\n",
       "         ...,\n",
       "         [ 0.1538,  1.0753, -0.8198,  ..., -0.9095, -0.5135, -0.9269],\n",
       "         [-0.3386,  0.3896, -0.3750,  ..., -1.0964, -0.2070, -1.2136],\n",
       "         [ 0.2793,  0.8052, -0.7451,  ..., -1.0058, -0.9107, -0.6302]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.3568,  1.1511, -1.1231,  ..., -0.7617,  0.0600, -1.1169],\n",
       "         [ 0.8038,  0.7970, -0.5940,  ..., -0.6540, -0.2263, -1.2037],\n",
       "         [ 1.1718, -0.6345, -0.5490,  ..., -0.8839, -0.9250, -0.5816],\n",
       "         ...,\n",
       "         [-0.3973,  0.6939, -0.8380,  ..., -0.4988,  0.0424, -1.4595],\n",
       "         [ 0.0054,  1.0076, -0.6244,  ..., -1.0507, -0.8749, -0.7948],\n",
       "         [ 0.0887,  0.3541, -0.5935,  ..., -1.1189, -1.2744, -0.3810]],\n",
       "\n",
       "        [[ 0.0901,  1.2458, -0.8769,  ..., -1.1107, -0.6387, -0.6220],\n",
       "         [ 0.4248,  1.3471, -1.0788,  ..., -1.0724, -0.6355, -0.8557],\n",
       "         [ 0.2040,  0.5704, -0.8476,  ..., -1.0467, -1.2305, -0.4593],\n",
       "         ...,\n",
       "         [-0.1553,  1.4072, -1.2028,  ..., -1.2485,  0.0190, -0.6077],\n",
       "         [-0.2104,  1.2620, -0.7495,  ..., -1.2373, -0.6365, -0.7523],\n",
       "         [ 0.2315,  1.0141, -0.7460,  ..., -1.0831, -1.2467, -0.4646]],\n",
       "\n",
       "        [[ 0.0536,  1.4354, -1.1175,  ..., -1.0066, -0.5429, -0.5176],\n",
       "         [ 0.4249,  1.6422, -0.8190,  ..., -0.7108, -0.5696, -1.0897],\n",
       "         [ 1.0371, -0.6059, -1.5253,  ..., -0.8226, -0.1565, -0.2732],\n",
       "         ...,\n",
       "         [-0.2894,  1.4363, -0.4864,  ..., -0.9391, -0.6291, -1.2151],\n",
       "         [-0.3820,  1.2392, -1.0213,  ..., -0.7508,  0.5208, -1.5633],\n",
       "         [ 0.0386,  1.1971, -0.8104,  ..., -0.9570, -1.2940, -0.3799]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr = 0.01)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.0529e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.6961e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.8087e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(9.7624e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.8097e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.3212e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.9469e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.6626e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(8.4709e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.3907e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(7.6793e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4803e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9041e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1233e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.7121e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.0935e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.1042e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.9463e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.8292e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.4193e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5836e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(6.2011e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.3639e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.5073e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0895e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9271e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.9861e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.0781e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3320e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4248e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.4226e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.4234e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.3627e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1575e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.1223e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.7347e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.5712e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(3.5520e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor(4.0789e-05, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "d_model = 8\n",
    "n_head = 1\n",
    "for epoch in range(num_epochs):\n",
    "    output1 = encoder(df_tensor)\n",
    "    output2 = encoder(Matrix_tensor)\n",
    "    loss = loss_func(output1,output2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([126, 8, 8])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Embedding_Netwokr.csv',output1.data.numpy().reshape(126,-1),delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
